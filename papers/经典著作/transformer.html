<!doctype html><html lang="zh-CN"><head><meta charset="utf-8" /><meta name="viewport" content="width=device-width,initial-scale=1" /><meta name="generator" content="VuePress 2.0.0-rc.23" /><meta name="theme" content="VuePress Theme Plume 1.0.0-rc.155" /><script id="check-mac-os">document.documentElement.classList.toggle('mac', /Mac|iPhone|iPod|iPad/i.test(navigator.platform))</script><script id="check-dark-mode">;(function () {const um= localStorage.getItem('vuepress-theme-appearance') || 'auto';const sm = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;const isDark = um === 'dark' || (um !== 'light' && sm);document.documentElement.dataset.theme = isDark ? 'dark' : 'light';})();</script><script type="application/ld+json">{"@context":"https://schema.org","@type":"Article","headline":"Attention Is All You Need","image":[""],"dateModified":"2025-05-25T13:13:36.000Z","author":[]}</script><meta property="og:url" content="https://velvet-notes.org/papers/%E7%BB%8F%E5%85%B8%E8%91%97%E4%BD%9C/transformer"><meta property="og:site_name" content="Velvet-Notes"><meta property="og:title" content="Attention Is All You Need"><meta property="og:description" content="Transformer 概述 Transformer 解决的问题 Transformer 的 Tips Transformer 的模型结构如下："><meta property="og:type" content="article"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2025-05-25T13:13:36.000Z"><meta property="article:tag" content="Self-Attention"><meta property="article:tag" content="Transformer"><meta property="article:modified_time" content="2025-05-25T13:13:36.000Z"><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"><title>Attention Is All You Need | Velvet-Notes</title><meta name="description" content="Transformer 概述 Transformer 解决的问题 Transformer 的 Tips Transformer 的模型结构如下："><link rel="preload" href="/assets/style-BGE8C2tN.css" as="style"><link rel="stylesheet" href="/assets/style-BGE8C2tN.css"><link rel="modulepreload" href="/assets/app-nJSYOaSR.js"><link rel="modulepreload" href="/assets/transformer.html-D6Vor5lw.js"><link rel="prefetch" href="/assets/index.html-DpWD9FS0.js" as="script"><link rel="prefetch" href="/assets/index.html-XI02li_r.js" as="script"><link rel="prefetch" href="/assets/envtips.html-G0X9nd2r.js" as="script"><link rel="prefetch" href="/assets/index.html-2zqwryLw.js" as="script"><link rel="prefetch" href="/assets/index.html-ixjxCYiW.js" as="script"><link rel="prefetch" href="/assets/index.html-Bl9idUjm.js" as="script"><link rel="prefetch" href="/assets/index.html-CABPt5Pp.js" as="script"><link rel="prefetch" href="/assets/index.html-uJRHY0H6.js" as="script"><link rel="prefetch" href="/assets/index.html-B6mq4iEn.js" as="script"><link rel="prefetch" href="/assets/index.html-DXZVIx1b.js" as="script"><link rel="prefetch" href="/assets/index.html-1xKjEa72.js" as="script"><link rel="prefetch" href="/assets/frag2seq.html-BNlp20Lo.js" as="script"><link rel="prefetch" href="/assets/ipdiff.html-CnGOPjd8.js" as="script"><link rel="prefetch" href="/assets/pocketgen.html-CgZjAkqj.js" as="script"><link rel="prefetch" href="/assets/targetdiff.html-DJ4ChdEc.js" as="script"><link rel="prefetch" href="/assets/ddpm.html-BV2yRF5A.js" as="script"><link rel="prefetch" href="/assets/resnet.html-BEZsIIkT.js" as="script"><link rel="prefetch" href="/assets/404.html-CKywGOP_.js" as="script"><link rel="prefetch" href="/assets/index.html-B1XVt5iZ.js" as="script"><link rel="prefetch" href="/assets/index.html-B4z6VAdw.js" as="script"><link rel="prefetch" href="/assets/index.html-cM78LN2l.js" as="script"><link rel="prefetch" href="/assets/index.html-ThqWo8Sh.js" as="script"><link rel="prefetch" href="/assets/photoswipe.esm-D2Nf-uDI.js" as="script"><link rel="prefetch" href="/assets/giscus-1zs_z9NH.js" as="script"><link rel="prefetch" href="/assets/searchBox-default-oZm0hqud.js" as="script"><link rel="prefetch" href="/assets/SearchBox-fA3meHCe.js" as="script"></head><body><div id="app"><!--[--><!--[--><div class="theme-plume vp-layout" vp-container data-v-c0907686><!--[--><!--[--><!--]--><!--[--><span tabindex="-1" data-v-1224062b></span><a href="#VPContent" class="vp-skip-link visually-hidden" data-v-1224062b> Skip to content </a><!--]--><!----><header class="vp-nav" data-v-c0907686 data-v-0d7c8d3b><div class="vp-navbar" vp-navbar data-v-0d7c8d3b data-v-c3e1f628><div class="wrapper" data-v-c3e1f628><div class="container" data-v-c3e1f628><div class="title" data-v-c3e1f628><div class="vp-navbar-title" data-v-c3e1f628 data-v-7547ac60><a class="vp-link link no-icon title" href="/" data-v-7547ac60><!--[--><!--[--><!--]--><!--[--><!--[--><!--[--><img class="vp-image dark logo" style="" src="https://theme-plume.vuejs.press/plume.png" alt data-v-6b7ad23c><!--]--><!--[--><img class="vp-image light logo" style="" src="https://theme-plume.vuejs.press/plume.png" alt data-v-6b7ad23c><!--]--><!--]--><!--]--><span data-v-7547ac60>Velvet-Notes</span><!--[--><!--]--><!--]--><!----></a></div></div><div class="content" data-v-c3e1f628><div class="content-body" data-v-c3e1f628><!--[--><!--]--><div class="vp-navbar-search search" data-v-c3e1f628><div class="search-wrapper" data-v-617be9c3><!----><div id="local-search" data-v-617be9c3><button type="button" class="mini-search mini-search-button" aria-label="搜索文档" data-v-617be9c3><span class="mini-search-button-container"><span class="mini-search-search-icon vpi-mini-search" aria-label="search icon"></span><span class="mini-search-button-placeholder">搜索文档</span></span><span class="mini-search-button-keys"><kbd class="mini-search-button-key"></kbd><kbd class="mini-search-button-key">K</kbd></span></button></div></div></div><!--[--><!--]--><nav aria-labelledby="main-nav-aria-label" class="vp-navbar-menu menu" data-v-c3e1f628 data-v-2c554402><span id="main-nav-aria-label" class="visually-hidden" data-v-2c554402>Main Navigation</span><!--[--><!--[--><a class="vp-link link navbar-menu-link" href="/" tabindex="0" data-v-2c554402 data-v-0f8fa3a5><!--[--><!----><span data-v-0f8fa3a5>首页</span><!----><!--]--><!----></a><!--]--><!--[--><a class="vp-link link navbar-menu-link" href="/projlist/" tabindex="0" data-v-2c554402 data-v-0f8fa3a5><!--[--><!----><span data-v-0f8fa3a5>项目</span><!----><!--]--><!----></a><!--]--><!--[--><a class="vp-link link navbar-menu-link" href="/blog/" tabindex="0" data-v-2c554402 data-v-0f8fa3a5><!--[--><!----><span data-v-0f8fa3a5>博客</span><!----><!--]--><!----></a><!--]--><!--[--><div class="vp-flyout vp-navbar-menu-group" data-v-2c554402 data-v-81423e5a><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-81423e5a><span class="text" data-v-81423e5a><!----><!----><span data-v-81423e5a>灵感</span><!----><span class="vpi-chevron-down text-icon" data-v-81423e5a></span></span></button><div class="menu" data-v-81423e5a><div class="vp-menu" data-v-81423e5a data-v-3ea3c7a2><div class="items" data-v-3ea3c7a2><!--[--><!--[--><div class="vp-menu-group" data-v-3ea3c7a2 data-v-d0e2eee7><p class="title" data-v-d0e2eee7><!----><span data-v-d0e2eee7>Papers</span></p><!--[--><!--[--><div class="vp-menu-link" data-v-d0e2eee7 data-v-2646c5da><a class="vp-link link" href="https://arxiv.org/" target="_blank" rel="noreferrer" data-v-2646c5da><!--[--><!----> arXiv.org e-Print archive <!----><!--]--><span class="vpi-external-link"></span></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-d0e2eee7 data-v-2646c5da><a class="vp-link link" href="https://www.connectedpapers.com/" target="_blank" rel="noreferrer" data-v-2646c5da><!--[--><!----> CONNECTED PAPERS <!----><!--]--><span class="vpi-external-link"></span></a></div><!--]--><!--]--></div><!--]--><!--[--><div class="vp-menu-group" data-v-3ea3c7a2 data-v-d0e2eee7><p class="title" data-v-d0e2eee7><!----><span data-v-d0e2eee7>Insights</span></p><!--[--><!--[--><div class="vp-menu-link" data-v-d0e2eee7 data-v-2646c5da><a class="vp-link link" href="https://soarxiv.org/" target="_blank" rel="noreferrer" data-v-2646c5da><!--[--><!----> soarXiv - the universe of science <!----><!--]--><span class="vpi-external-link"></span></a></div><!--]--><!--]--></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="vp-flyout vp-navbar-menu-group" data-v-2c554402 data-v-81423e5a><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-81423e5a><span class="text" data-v-81423e5a><!----><!----><span data-v-81423e5a>笔记</span><!----><span class="vpi-chevron-down text-icon" data-v-81423e5a></span></span></button><div class="menu" data-v-81423e5a><div class="vp-menu" data-v-81423e5a data-v-3ea3c7a2><div class="items" data-v-3ea3c7a2><!--[--><!--[--><div class="vp-menu-group" data-v-3ea3c7a2 data-v-d0e2eee7><!----><!--[--><!--[--><div class="vp-menu-link" data-v-d0e2eee7 data-v-2646c5da><a class="vp-link link" href="/Memo/" data-v-2646c5da><!--[--><!----> 备忘录 <!----><!--]--><!----></a></div><!--]--><!--[--><div class="vp-menu-link" data-v-d0e2eee7 data-v-2646c5da><a class="vp-link link" href="/D2L/" data-v-2646c5da><!--[--><!----> Dive into Deep Learning <!----><!--]--><!----></a></div><!--]--><!--]--></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--]--></nav><!--[--><!--]--><!----><div class="vp-navbar-appearance appearance" data-v-c3e1f628 data-v-ca84e07a><button class="vp-switch vp-switch-appearance" type="button" role="switch" title aria-checked="false" data-v-ca84e07a data-v-4fb4d2cf data-v-668778a2><span class="check" data-v-668778a2><span class="icon" data-v-668778a2><!--[--><span class="vpi-sun sun" data-v-4fb4d2cf></span><span class="vpi-moon moon" data-v-4fb4d2cf></span><!--]--></span></span></button></div><div class="vp-social-links vp-navbar-social-links social-links" data-v-c3e1f628 data-v-b24c63ab data-v-e2339aca><!--[--><a class="vp-social-link no-icon" href="https://github.com/Velvet0314" aria-label="github" target="_blank" rel="noopener" data-v-e2339aca data-v-675ac7ab><span class="vpi-social-github" /></a><!--]--></div><div class="vp-flyout vp-navbar-extra extra" data-v-c3e1f628 data-v-385b85c2 data-v-81423e5a><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-81423e5a><span class="vpi-more-horizontal icon" data-v-81423e5a></span></button><div class="menu" data-v-81423e5a><div class="vp-menu" data-v-81423e5a data-v-3ea3c7a2><!----><!--[--><!--[--><!----><div class="group" data-v-385b85c2><div class="item appearance" data-v-385b85c2><p class="label" data-v-385b85c2>外观</p><div class="appearance-action" data-v-385b85c2><button class="vp-switch vp-switch-appearance" type="button" role="switch" title aria-checked="false" data-v-385b85c2 data-v-4fb4d2cf data-v-668778a2><span class="check" data-v-668778a2><span class="icon" data-v-668778a2><!--[--><span class="vpi-sun sun" data-v-4fb4d2cf></span><span class="vpi-moon moon" data-v-4fb4d2cf></span><!--]--></span></span></button></div></div></div><div class="group" data-v-385b85c2><div class="item social-links" data-v-385b85c2><div class="vp-social-links social-links-list" data-v-385b85c2 data-v-e2339aca><!--[--><a class="vp-social-link no-icon" href="https://github.com/Velvet0314" aria-label="github" target="_blank" rel="noopener" data-v-e2339aca data-v-675ac7ab><span class="vpi-social-github" /></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="vp-navbar-hamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="nav-screen" data-v-c3e1f628 data-v-a01879b9><span class="container" data-v-a01879b9><span class="top" data-v-a01879b9></span><span class="middle" data-v-a01879b9></span><span class="bottom" data-v-a01879b9></span></span></button></div></div></div></div><div class="divider" data-v-c3e1f628><div class="divider-line" data-v-c3e1f628></div></div></div><!----></header><div class="vp-local-nav fixed reached-top is-blog" data-v-c0907686 data-v-3ea7a05f><button class="hidden menu" disabled aria-expanded="false" aria-controls="SidebarNav" data-v-3ea7a05f><span class="vpi-align-left menu-icon" data-v-3ea7a05f></span><span class="menu-text" data-v-3ea7a05f>Menu</span></button><div class="vp-local-nav-outline-dropdown" style="--vp-vh:0px;" data-v-3ea7a05f data-v-89d45550><button data-v-89d45550>返回顶部</button><!----></div></div><!----><!--[--><div id="VPContent" vp-content class="vp-content" data-v-c0907686 data-v-8e6b0b1c><div class="vp-doc-container is-blog" data-v-8e6b0b1c data-v-ce8ab5ff><!--[--><!--]--><div class="container" data-v-ce8ab5ff><!----><div class="content" data-v-ce8ab5ff><div class="content-container" data-v-ce8ab5ff><!--[--><!--]--><main class="main" data-v-ce8ab5ff><nav class="vp-breadcrumb" data-v-ce8ab5ff data-v-f43771b3><ol vocab="https://schema.org/" typeof="BreadcrumbList" data-v-f43771b3><!--[--><li property="itemListElement" typeof="ListItem" data-v-f43771b3><a class="vp-link link breadcrumb" href="/" property="item" typeof="WebPage" data-v-f43771b3><!--[-->首页<!--]--><!----></a><span class="vpi-chevron-right" data-v-f43771b3></span><meta property="name" content="首页" data-v-f43771b3><meta property="position" content="1" data-v-f43771b3></li><li property="itemListElement" typeof="ListItem" data-v-f43771b3><a class="vp-link link breadcrumb" href="/blog/" property="item" typeof="WebPage" data-v-f43771b3><!--[-->博客<!--]--><!----></a><span class="vpi-chevron-right" data-v-f43771b3></span><meta property="name" content="博客" data-v-f43771b3><meta property="position" content="2" data-v-f43771b3></li><li property="itemListElement" typeof="ListItem" data-v-f43771b3><a class="vp-link link breadcrumb" href="/blog/categories/?id=a566b2" property="item" typeof="WebPage" data-v-f43771b3><!--[-->papers<!--]--><!----></a><span class="vpi-chevron-right" data-v-f43771b3></span><meta property="name" content="papers" data-v-f43771b3><meta property="position" content="3" data-v-f43771b3></li><li property="itemListElement" typeof="ListItem" data-v-f43771b3><a class="vp-link link breadcrumb" href="/blog/categories/?id=3111f2" property="item" typeof="WebPage" data-v-f43771b3><!--[-->经典著作<!--]--><!----></a><span class="vpi-chevron-right" data-v-f43771b3></span><meta property="name" content="经典著作" data-v-f43771b3><meta property="position" content="4" data-v-f43771b3></li><li property="itemListElement" typeof="ListItem" data-v-f43771b3><a class="vp-link link breadcrumb current" href="/papers/%E7%BB%8F%E5%85%B8%E8%91%97%E4%BD%9C/transformer" property="item" typeof="WebPage" data-v-f43771b3><!--[-->Attention Is All You Need<!--]--><!----></a><!----><meta property="name" content="Attention Is All You Need" data-v-f43771b3><meta property="position" content="5" data-v-f43771b3></li><!--]--></ol></nav><!--[--><!--]--><!--[--><h1 class="vp-doc-title page-title" data-v-b15de072>Attention Is All You Need <!----></h1><div class="vp-doc-meta" data-v-b15de072><!--[--><!--]--><p class="reading-time" data-v-b15de072><span class="vpi-books icon" data-v-b15de072></span><span data-v-b15de072>约 1267 字</span><span data-v-b15de072>大约 4 分钟</span></p><p data-v-b15de072><span class="vpi-tag icon" data-v-b15de072></span><!--[--><a class="vp-link link tag vp-tag-4awj" href="/blog/tags/?tag=Transformer" data-v-b15de072><!--[-->Transformer<!--]--><!----></a><a class="vp-link link tag vp-tag-rac4" href="/blog/tags/?tag=Self-Attention" data-v-b15de072><!--[-->Self-Attention<!--]--><!----></a><!--]--></p><!--[--><!--]--><p class="create-time" data-v-b15de072><span class="vpi-clock icon" data-v-b15de072></span><span data-v-b15de072>2025-05-08</span></p></div><!--]--><!--[--><!--]--><div class="_papers_%E7%BB%8F%E5%85%B8%E8%91%97%E4%BD%9C_transformer external-link-icon-enabled vp-doc plume-content" vp-content data-v-ce8ab5ff><!--[--><!--]--><div data-v-ce8ab5ff><h2 id="transformer-概述" tabindex="-1"><a class="header-anchor" href="#transformer-概述"><span><strong>Transformer 概述</strong></span></a></h2><h3 id="transformer-解决的问题" tabindex="-1"><a class="header-anchor" href="#transformer-解决的问题"><span><strong>Transformer 解决的问题</strong></span></a></h3><table><thead><tr><th>🚀 问题类别</th><th>❌ 传统方法的局限</th><th>✅ Transformer 的解决方案</th></tr></thead><tbody><tr><td><strong>长距离依赖建模</strong></td><td>🚨 RNN 难以捕捉长距离上下文，存在梯度消失/爆炸问题</td><td>✨ 采用自注意力机制，直接建立所有词之间的依赖关系</td></tr><tr><td><strong>并行化效率</strong></td><td>🚨 RNN 的时序依赖性导致难以并行，训练时间较长</td><td>✨ 全注意力架构支持高度并行，显著提升训练速度</td></tr><tr><td><strong>模型结构复杂度</strong></td><td>🚨 复杂的 RNN 结构堆叠（如 LSTM+Attention）增加了计算和设计难度</td><td>✨ 采用统一的注意力模块（Self-Attention），简化模型架构</td></tr><tr><td><strong>全局建模能力</strong></td><td>🚨 注意力机制作为辅助模块，受限于 RNN/CNN 框架，无法充分利用全局信息</td><td>✨ 纯注意力结构全局化信息建模，灵活捕捉各词之间的相互依赖</td></tr></tbody></table><h2 id="transformer-的-tips" tabindex="-1"><a class="header-anchor" href="#transformer-的-tips"><span><strong>Transformer 的 Tips</strong></span></a></h2><p>Transformer 的模型结构如下：</p><div class="vp-image-card center" style="width:65%;" data-v-4316d14c><div class="image-container" data-v-4316d14c><img src="https://image.velvet-notes.org/blog/transformer_structure.png" loading="lazy" data-v-4316d14c><!----></div></div><h3 id="tip-1-编码器-解码器架构堆叠-encoder-and-decoder-stacks" tabindex="-1"><a class="header-anchor" href="#tip-1-编码器-解码器架构堆叠-encoder-and-decoder-stacks"><span><mark class="note"><strong>Tip 1：编码器-解码器架构堆叠（Encoder and Decoder Stacks）</strong></mark></span></a></h3><h4 id="encoder" tabindex="-1"><a class="header-anchor" href="#encoder"><span><strong>Encoder</strong></span></a></h4><p><strong>⭐整体结构：</strong></p><p>由 <strong>6层（N=6）完全相同的层堆叠</strong> 而成，每一层都有两个 <strong>子层（sub-layer）</strong>：</p><p><mark class="note">1. <strong>多头自注意力机制（Multi-Head Self-Attention）：</strong></mark></p><ul><li>输入序列中的每个词（Token）可以与其他词建立依赖关系</li><li>通过多个注意力头并行计算，可以捕捉<mark class="important">不同的依赖模式（多头学习多表征）</mark></li></ul><p><mark class="note">2. <strong>位置编码的全连接前馈网络（Position-wise Fully Connected Feed-Forward Network, FFN）：</strong></mark></p><ul><li>对每个位置（Token）单独应用相同的前馈神经网络，<mark class="important">通过两个线性层映射到需要的语义空间中</mark>，其中在输入 Embedding 后进行 <strong>位置编码（Positional Encoding）</strong></li></ul><p><mark class="important">Encoder 的输入是 <strong>原始序列</strong></mark></p><p><strong>⭐残差连接与层归一化（Layer Normalization）：</strong></p><p><strong>残差连接（Residual Connection）</strong> —— 每个子层输出都会与输入直接相加，同时对每个子层的输出进行归一化，稳定训练过程：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>LayerNorm</mtext><mo stretchy="false">(</mo><mi>x</mi><mo>+</mo><mtext>Sublayer</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{LayerNorm}(x + \text{Sublayer}(x)) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">LayerNorm</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Sublayer</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))</span></span></span></span></span></p><p>这种设计缓解了梯度消失问题，并使梯度更稳定</p><p><strong>⭐输出维度：</strong> 每一层的输出都保持相同的维度 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mo>=</mo><mn>512</mn></mrow><annotation encoding="application/x-tex">d_{model} = 512</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">512</span></span></span></span>，这确保了模型结构的简洁性</p><h4 id="decoder" tabindex="-1"><a class="header-anchor" href="#decoder"><span><strong>Decoder</strong></span></a></h4><p><strong>⭐整体结构：</strong></p><p>也由 <strong>6层（N=6）完全相同的层堆叠</strong> 而成，每层有三个子层：</p><p><mark class="note">1. <strong>掩码多头自注意力机制（Masked Multi-Head Self-Attention）：</strong></mark></p><p>与Encoder的自注意力类似，但引入了 <b>&quot;掩码&quot;（Mask）</b> 机制</p><p>掩码确保了位置 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span> 的词只能看到它之前 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>→</mo><mi>i</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1 \to i-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7429em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span> 的词，避免在训练过程中泄漏未来信息</p><div class="hint-container note"><p class="hint-container-title">关于 Mask</p><p><mark class="important">Self-Attention 是能够看到全部的输入（所有的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">K</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">y</mi></mrow><mo separator="true">,</mo><mrow><mi mathvariant="normal">V</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">e</mi></mrow></mrow><annotation encoding="application/x-tex">\mathrm{Key}, \mathrm{Value}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathrm" style="margin-right:0.01389em;">Key</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathrm">Value</span></span></span></span></span>），为了保持训练与预测的一致（自回归模型，在时刻 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span> 只用已生成（或已知）的前序信息去预测下一个词），使用 Mask 只允许其看到 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span> 时刻之前的信息</mark></p><p><strong>⭐Mask 的操作原理：</strong></p><p>由于在自注意力中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Q</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">y</mi></mrow><annotation encoding="application/x-tex">\mathrm{Query}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathrm" style="margin-right:0.01389em;">Query</span></span></span></span></span> 会与每个 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">K</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">y</mi></mrow><annotation encoding="application/x-tex">\mathrm{Key}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathrm" style="margin-right:0.01389em;">Key</span></span></span></span></span> 进行计算，但是为了保持训练与预测的一致性，要求当前的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi mathvariant="normal">K</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">y</mi></mrow><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\mathrm{Key}_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9275em;vertical-align:-0.2441em;"></span><span class="mord"><span class="mord"><span class="mord mathrm" style="margin-right:0.01389em;">Key</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1864em;"><span style="top:-2.4559em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em;"><span></span></span></span></span></span></span></span></span></span> 只能取 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">K</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">y</mi></mrow><mn>1</mn><mo>→</mo><msub><mrow><mi mathvariant="normal">K</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">y</mi></mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\mathrm{Key}1 \to \mathrm{Key}_{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathrm" style="margin-right:0.01389em;">Key</span></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9858em;vertical-align:-0.3025em;"></span><span class="mord"><span class="mord"><span class="mord mathrm" style="margin-right:0.01389em;">Key</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.207em;"><span style="top:-2.4559em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3025em;"><span></span></span></span></span></span></span></span></span></span></p><p>Mask 通过对 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi mathvariant="normal">K</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">y</mi></mrow><mi>t</mi></msub><mo>⋯</mo></mrow><annotation encoding="application/x-tex">\mathrm{Key}_t \cdots</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9275em;vertical-align:-0.2441em;"></span><span class="mord"><span class="mord"><span class="mord mathrm" style="margin-right:0.01389em;">Key</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1864em;"><span style="top:-2.4559em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">⋯</span></span></span></span> 以后的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">K</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">y</mi></mrow><annotation encoding="application/x-tex">\mathrm{Key}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathrm" style="margin-right:0.01389em;">Key</span></span></span></span></span> 取一个非常大的负数，使得经 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">x</mi></mrow><annotation encoding="application/x-tex">\mathrm{softmax}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord"><span class="mord mathrm">softmax</span></span></span></span></span> 后这些结果都是 0</p></div><p><mark class="note">2. <strong>多头非自注意力（Multi-Head Attention）：</strong></mark></p><ul><li>将编码器的输出作为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">K</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">y</mi></mrow><annotation encoding="application/x-tex">\mathrm{Key}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathrm" style="margin-right:0.01389em;">Key</span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">V</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">e</mi></mrow><annotation encoding="application/x-tex">\mathrm{Value}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord"><span class="mord mathrm">Value</span></span></span></span></span>，而 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Q</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">y</mi></mrow><annotation encoding="application/x-tex">\mathrm{Query}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathrm" style="margin-right:0.01389em;">Query</span></span></span></span></span> 来自解码器的前一层的输出。<mark class="important">这让 Decoder 能获取 Encoder 编码的全局信息（源序列）作为监督</mark></li><li>将编码器输出作为键值，相当于为解码器提供静态的全局记忆（Static Memory），而查询向量则代表动态生成过程中的当前状态。这种&quot;动态查询+静态记忆&quot;的组合，既满足自回归的因果约束，又能充分挖掘源序列信息</li></ul><p><mark class="note">3. <strong>Feed-Forward Network (FFN)：</strong></mark></p><ul><li>与 Encoder 中的 FFN 相同</li></ul><p><strong>残差连接和层归一化：</strong></p><p>和 Encoder 一致，每个子层都有 <strong>残差连接</strong> 和 <strong>层归一化</strong> ，确保训练稳定</p><ul><li><mark class="important">在训练中，Decoder 的输入是 <strong>目标序列</strong> 经过 <strong>右移（shift right）</strong> 的一个序列</mark></li></ul><div class="hint-container note"><p class="hint-container-title">关于 Teacher Forcing</p><p>使用真实的 <strong>目标序列（Ground Truth）</strong> 作为输入，以加速收敛并稳定训练过程</p></div><div class="hint-container note"><p class="hint-container-title">关于 Scheduled Sampling</p><p>有些任务需要一些 <strong>随机性（noise）</strong>，错误的样本会得到更好的训练效果（泛用性）</p><p>如果单纯使用 Teacher Forcing，由于训练和推断时 Decoder 输入不一致导致 <strong>曝光偏差（Exposure Bias）</strong></p></div><ul><li><mark class="important">在推理中，Decoder 的输入是 <strong>前面生成的序列</strong></mark></li></ul><div class="hint-container tip"><p class="hint-container-title">为什么要 &quot;Shift Right&quot;?</p><p>为了易于预测第一个 token（对于第一个 token 需要一个类似于占位符的存在）</p></div><p>Transformer 的训练流程如下（with teacher forcing）：</p><div class="vp-image-card center" style="width:60%;" data-v-4316d14c><div class="image-container" data-v-4316d14c><img src="https://image.velvet-notes.org/blog/transformer_process.png" loading="lazy" data-v-4316d14c><!----></div></div></div><!--[--><h2 id="doc-contributors" tabindex="-1"><a href="#doc-contributors" class="header-anchor"><span>贡献者</span></a></h2><div class="vp-contributors"><a href="https://github.com/Velvet0314" target="_blank" rel="noreferrer" class="vp-contributor"><img src="https://image.velvet-notes.org/blog/avatar.png" alt class="vp-contributor-avatar"><span class="vp-contributor-name">Velvet</span></a></div><!--]--><!----><div class="vp-doc-copyright" data-v-ce8ab5ff><h2 id="doc-copyright" tabindex="-1" class="vp-doc-header" data-v-8f9b9288><a href="#doc-copyright" class="header-anchor" data-v-8f9b9288><span data-v-8f9b9288><!--[-->版权所有<!--]--></span></a></h2><div class="hint-container tip copyright-container" data-v-6e46fd1c><p data-v-6e46fd1c><span data-v-6e46fd1c>版权归属：</span><a class="vp-link link no-icon" href="https://github.com/Velvet0314" target="_blank" rel="noreferrer" data-v-6e46fd1c><!--[-->Velvet<!--]--><!----></a></p><!----><p data-v-6e46fd1c><span data-v-6e46fd1c>许可证：</span><a class="vp-link link no-icon" href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noreferrer" data-v-6e46fd1c><!--[-->署名-非商业性-相同方式共享 4.0 国际 (CC-BY-NC-SA-4.0)<!--]--><!----></a><!--[--><span class="vpi-license-cc" data-v-6e46fd1c></span><span class="vpi-license-by" data-v-6e46fd1c></span><span class="vpi-license-nc" data-v-6e46fd1c></span><span class="vpi-license-sa" data-v-6e46fd1c></span><!--]--></p></div></div></div></main><footer class="vp-doc-footer" data-v-ce8ab5ff data-v-d1aa847a><!--[--><!--]--><div class="edit-info" data-v-d1aa847a><div class="edit-link" data-v-d1aa847a><a class="vp-link link no-icon edit-link-button" href="https://github.com/Velvet0314/Velvet-Blog/edit/master/docs/papers/经典著作/transformer.md" target="_blank" rel="noreferrer" data-v-d1aa847a><!--[--><span class="vpi-square-pen edit-link-icon" aria-label="edit icon" data-v-d1aa847a></span> 编辑此页<!--]--><!----></a></div><!----></div><!----><nav class="prev-next" data-v-d1aa847a><div class="pager" data-v-d1aa847a><a class="vp-link link pager-link prev" href="/papers/%E7%BB%8F%E5%85%B8%E8%91%97%E4%BD%9C/resnet" data-v-d1aa847a><!--[--><span class="desc" data-v-d1aa847a>上一页</span><span class="title" data-v-d1aa847a>ResNet</span><!--]--><!----></a></div><div class="pager" data-v-d1aa847a><a class="vp-link link pager-link next" href="/papers/%E7%BB%8F%E5%85%B8%E8%91%97%E4%BD%9C/ddpm" data-v-d1aa847a><!--[--><span class="desc" data-v-d1aa847a>下一页</span><span class="title" data-v-d1aa847a>DDPM</span><!--]--><!----></a></div></nav></footer><div id="comment" class="giscus-wrapper input-top vp-comment" vp-comment style="display:block;" data-v-ce8ab5ff><div style="display: flex;align-items: center;justify-content: center;height: 96px"><span style="--loading-icon: url(&quot;data:image/svg+xml;utf8,%3Csvg xmlns=&#39;http://www.w3.org/2000/svg&#39; preserveAspectRatio=&#39;xMidYMid&#39; viewBox=&#39;25 25 50 50&#39;%3E%3CanimateTransform attributeName=&#39;transform&#39; type=&#39;rotate&#39; dur=&#39;2s&#39; keyTimes=&#39;0;1&#39; repeatCount=&#39;indefinite&#39; values=&#39;0;360&#39;%3E%3C/animateTransform%3E%3Ccircle cx=&#39;50&#39; cy=&#39;50&#39; r=&#39;20&#39; fill=&#39;none&#39; stroke=&#39;currentColor&#39; stroke-width=&#39;4&#39; stroke-linecap=&#39;round&#39;%3E%3Canimate attributeName=&#39;stroke-dasharray&#39; dur=&#39;1.5s&#39; keyTimes=&#39;0;0.5;1&#39; repeatCount=&#39;indefinite&#39; values=&#39;1,200;90,200;1,200&#39;%3E%3C/animate%3E%3Canimate attributeName=&#39;stroke-dashoffset&#39; dur=&#39;1.5s&#39; keyTimes=&#39;0;0.5;1&#39; repeatCount=&#39;indefinite&#39; values=&#39;0;-35px;-125px&#39;%3E%3C/animate%3E%3C/circle%3E%3C/svg%3E&quot;);--icon-size: 48px;display: inline-block;width: var(--icon-size);height: var(--icon-size);background-color: currentcolor;-webkit-mask-image: var(--loading-icon);mask-image: var(--loading-icon)"></span></div></div><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><!--]--><button style="display:none;" type="button" class="vp-back-to-top" aria-label="back to top" data-v-c0907686 data-v-fc8ff415><span class="icon vpi-back-to-top" data-v-fc8ff415></span><svg aria-hidden="true" data-v-fc8ff415><circle cx="50%" cy="50%" fill="none" style="stroke-dasharray:calc(0% - 12.566370614359172px) calc(314.1592653589793% - 12.566370614359172px);" data-v-fc8ff415></circle></svg></button><footer class="vp-footer" vp-footer data-v-c0907686 data-v-d288e6a5><!--[--><div class="container" data-v-d288e6a5><p class="message" data-v-d288e6a5>Powered by <a target="_blank" href="https://v2.vuepress.vuejs.org/">VuePress</a> & <a target="_blank" href="https://theme-plume.vuejs.press">vuepress-theme-plume</a></p><p class="copyright" data-v-d288e6a5>Copyright © 2024-present Velvet</p></div><!--]--></footer><!--[--><!--]--><!--]--></div><!----><!--]--><!--[--><!--]--><!--]--></div><script type="module" src="/assets/app-nJSYOaSR.js" defer></script></body></html>
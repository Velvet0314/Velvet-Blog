import{_ as m,c as e,d as t,b as n,r,o as i}from"./app-nJSYOaSR.js";const p={};function o(l,a){const s=r("ImageCard");return i(),e("div",null,[a[0]||(a[0]=t('<h2 id="transformer-概述" tabindex="-1"><a class="header-anchor" href="#transformer-概述"><span><strong>Transformer 概述</strong></span></a></h2><h3 id="transformer-解决的问题" tabindex="-1"><a class="header-anchor" href="#transformer-解决的问题"><span><strong>Transformer 解决的问题</strong></span></a></h3><table><thead><tr><th>🚀 问题类别</th><th>❌ 传统方法的局限</th><th>✅ Transformer 的解决方案</th></tr></thead><tbody><tr><td><strong>长距离依赖建模</strong></td><td>🚨 RNN 难以捕捉长距离上下文，存在梯度消失/爆炸问题</td><td>✨ 采用自注意力机制，直接建立所有词之间的依赖关系</td></tr><tr><td><strong>并行化效率</strong></td><td>🚨 RNN 的时序依赖性导致难以并行，训练时间较长</td><td>✨ 全注意力架构支持高度并行，显著提升训练速度</td></tr><tr><td><strong>模型结构复杂度</strong></td><td>🚨 复杂的 RNN 结构堆叠（如 LSTM+Attention）增加了计算和设计难度</td><td>✨ 采用统一的注意力模块（Self-Attention），简化模型架构</td></tr><tr><td><strong>全局建模能力</strong></td><td>🚨 注意力机制作为辅助模块，受限于 RNN/CNN 框架，无法充分利用全局信息</td><td>✨ 纯注意力结构全局化信息建模，灵活捕捉各词之间的相互依赖</td></tr></tbody></table><h2 id="transformer-的-tips" tabindex="-1"><a class="header-anchor" href="#transformer-的-tips"><span><strong>Transformer 的 Tips</strong></span></a></h2><p>Transformer 的模型结构如下：</p>',5)),n(s,{image:"https://image.velvet-notes.org/blog/transformer_structure.png",width:"65%",center:"true"}),a[1]||(a[1]=t('<h3 id="tip-1-编码器-解码器架构堆叠-encoder-and-decoder-stacks" tabindex="-1"><a class="header-anchor" href="#tip-1-编码器-解码器架构堆叠-encoder-and-decoder-stacks"><span><mark class="note"><strong>Tip 1：编码器-解码器架构堆叠（Encoder and Decoder Stacks）</strong></mark></span></a></h3><h4 id="encoder" tabindex="-1"><a class="header-anchor" href="#encoder"><span><strong>Encoder</strong></span></a></h4><p><strong>⭐整体结构：</strong></p><p>由 <strong>6层（N=6）完全相同的层堆叠</strong> 而成，每一层都有两个 <strong>子层（sub-layer）</strong>：</p><p><mark class="note">1. <strong>多头自注意力机制（Multi-Head Self-Attention）：</strong></mark></p><ul><li>输入序列中的每个词（Token）可以与其他词建立依赖关系</li><li>通过多个注意力头并行计算，可以捕捉<mark class="important">不同的依赖模式（多头学习多表征）</mark></li></ul><p><mark class="note">2. <strong>位置编码的全连接前馈网络（Position-wise Fully Connected Feed-Forward Network, FFN）：</strong></mark></p><ul><li>对每个位置（Token）单独应用相同的前馈神经网络，<mark class="important">通过两个线性层映射到需要的语义空间中</mark>，其中在输入 Embedding 后进行 <strong>位置编码（Positional Encoding）</strong></li></ul><p><mark class="important">Encoder 的输入是 <strong>原始序列</strong></mark></p><p><strong>⭐残差连接与层归一化（Layer Normalization）：</strong></p><p><strong>残差连接（Residual Connection）</strong> —— 每个子层输出都会与输入直接相加，同时对每个子层的输出进行归一化，稳定训练过程：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>LayerNorm</mtext><mo stretchy="false">(</mo><mi>x</mi><mo>+</mo><mtext>Sublayer</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\\text{LayerNorm}(x + \\text{Sublayer}(x)) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">LayerNorm</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">Sublayer</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))</span></span></span></span></span></p><p>这种设计缓解了梯度消失问题，并使梯度更稳定</p><p><strong>⭐输出维度：</strong> 每一层的输出都保持相同的维度 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow></msub><mo>=</mo><mn>512</mn></mrow><annotation encoding="application/x-tex">d_{model} = 512</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">512</span></span></span></span>，这确保了模型结构的简洁性</p><h4 id="decoder" tabindex="-1"><a class="header-anchor" href="#decoder"><span><strong>Decoder</strong></span></a></h4><p><strong>⭐整体结构：</strong></p><p>也由 <strong>6层（N=6）完全相同的层堆叠</strong> 而成，每层有三个子层：</p><p><mark class="note">1. <strong>掩码多头自注意力机制（Masked Multi-Head Self-Attention）：</strong></mark></p><p>与Encoder的自注意力类似，但引入了 <b>&quot;掩码&quot;（Mask）</b> 机制</p><p>掩码确保了位置 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6595em;"></span><span class="mord mathnormal">i</span></span></span></span> 的词只能看到它之前 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>→</mo><mi>i</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1 \\to i-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7429em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span> 的词，避免在训练过程中泄漏未来信息</p><div class="hint-container note"><p class="hint-container-title">关于 Mask</p><p><mark class="important">Self-Attention 是能够看到全部的输入（所有的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">K</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">y</mi></mrow><mo separator="true">,</mo><mrow><mi mathvariant="normal">V</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">e</mi></mrow></mrow><annotation encoding="application/x-tex">\\mathrm{Key}, \\mathrm{Value}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathrm" style="margin-right:0.01389em;">Key</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathrm">Value</span></span></span></span></span>），为了保持训练与预测的一致（自回归模型，在时刻 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span> 只用已生成（或已知）的前序信息去预测下一个词），使用 Mask 只允许其看到 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span> 时刻之前的信息</mark></p><p><strong>⭐Mask 的操作原理：</strong></p><p>由于在自注意力中，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Q</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">y</mi></mrow><annotation encoding="application/x-tex">\\mathrm{Query}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathrm" style="margin-right:0.01389em;">Query</span></span></span></span></span> 会与每个 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">K</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">y</mi></mrow><annotation encoding="application/x-tex">\\mathrm{Key}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathrm" style="margin-right:0.01389em;">Key</span></span></span></span></span> 进行计算，但是为了保持训练与预测的一致性，要求当前的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi mathvariant="normal">K</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">y</mi></mrow><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\\mathrm{Key}_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9275em;vertical-align:-0.2441em;"></span><span class="mord"><span class="mord"><span class="mord mathrm" style="margin-right:0.01389em;">Key</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1864em;"><span style="top:-2.4559em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em;"><span></span></span></span></span></span></span></span></span></span> 只能取 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">K</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">y</mi></mrow><mn>1</mn><mo>→</mo><msub><mrow><mi mathvariant="normal">K</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">y</mi></mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\\mathrm{Key}1 \\to \\mathrm{Key}_{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathrm" style="margin-right:0.01389em;">Key</span></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9858em;vertical-align:-0.3025em;"></span><span class="mord"><span class="mord"><span class="mord mathrm" style="margin-right:0.01389em;">Key</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.207em;"><span style="top:-2.4559em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3025em;"><span></span></span></span></span></span></span></span></span></span></p><p>Mask 通过对 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mrow><mi mathvariant="normal">K</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">y</mi></mrow><mi>t</mi></msub><mo>⋯</mo></mrow><annotation encoding="application/x-tex">\\mathrm{Key}_t \\cdots</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9275em;vertical-align:-0.2441em;"></span><span class="mord"><span class="mord"><span class="mord mathrm" style="margin-right:0.01389em;">Key</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1864em;"><span style="top:-2.4559em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2441em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">⋯</span></span></span></span> 以后的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">K</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">y</mi></mrow><annotation encoding="application/x-tex">\\mathrm{Key}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathrm" style="margin-right:0.01389em;">Key</span></span></span></span></span> 取一个非常大的负数，使得经 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">x</mi></mrow><annotation encoding="application/x-tex">\\mathrm{softmax}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord"><span class="mord mathrm">softmax</span></span></span></span></span> 后这些结果都是 0</p></div><p><mark class="note">2. <strong>多头非自注意力（Multi-Head Attention）：</strong></mark></p><ul><li>将编码器的输出作为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">K</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">y</mi></mrow><annotation encoding="application/x-tex">\\mathrm{Key}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathrm" style="margin-right:0.01389em;">Key</span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">V</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">e</mi></mrow><annotation encoding="application/x-tex">\\mathrm{Value}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord"><span class="mord mathrm">Value</span></span></span></span></span>，而 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Q</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">y</mi></mrow><annotation encoding="application/x-tex">\\mathrm{Query}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathrm" style="margin-right:0.01389em;">Query</span></span></span></span></span> 来自解码器的前一层的输出。<mark class="important">这让 Decoder 能获取 Encoder 编码的全局信息（源序列）作为监督</mark></li><li>将编码器输出作为键值，相当于为解码器提供静态的全局记忆（Static Memory），而查询向量则代表动态生成过程中的当前状态。这种&quot;动态查询+静态记忆&quot;的组合，既满足自回归的因果约束，又能充分挖掘源序列信息</li></ul><p><mark class="note">3. <strong>Feed-Forward Network (FFN)：</strong></mark></p><ul><li>与 Encoder 中的 FFN 相同</li></ul><p><strong>残差连接和层归一化：</strong></p><p>和 Encoder 一致，每个子层都有 <strong>残差连接</strong> 和 <strong>层归一化</strong> ，确保训练稳定</p><ul><li><mark class="important">在训练中，Decoder 的输入是 <strong>目标序列</strong> 经过 <strong>右移（shift right）</strong> 的一个序列</mark></li></ul><div class="hint-container note"><p class="hint-container-title">关于 Teacher Forcing</p><p>使用真实的 <strong>目标序列（Ground Truth）</strong> 作为输入，以加速收敛并稳定训练过程</p></div><div class="hint-container note"><p class="hint-container-title">关于 Scheduled Sampling</p><p>有些任务需要一些 <strong>随机性（noise）</strong>，错误的样本会得到更好的训练效果（泛用性）</p><p>如果单纯使用 Teacher Forcing，由于训练和推断时 Decoder 输入不一致导致 <strong>曝光偏差（Exposure Bias）</strong></p></div><ul><li><mark class="important">在推理中，Decoder 的输入是 <strong>前面生成的序列</strong></mark></li></ul><div class="hint-container tip"><p class="hint-container-title">为什么要 &quot;Shift Right&quot;?</p><p>为了易于预测第一个 token（对于第一个 token 需要一个类似于占位符的存在）</p></div><p>Transformer 的训练流程如下（with teacher forcing）：</p>',33)),n(s,{image:"https://image.velvet-notes.org/blog/transformer_process.png",width:"60%",center:"true"})])}const h=m(p,[["render",o]]),g=JSON.parse('{"path":"/papers/%E7%BB%8F%E5%85%B8%E8%91%97%E4%BD%9C/transformer","title":"Attention Is All You Need","lang":"zh-CN","frontmatter":{"title":"Attention Is All You Need","createTime":"2025/05/08 16:54:34","tags":["Transformer","Self-Attention"],"permalink":"/papers/经典著作/transformer","prev":"/papers/经典著作/resnet","next":"/papers/经典著作/ddpm","outline":[2,4],"description":"Transformer 概述 Transformer 解决的问题 Transformer 的 Tips Transformer 的模型结构如下：","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Attention Is All You Need\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-05-25T13:13:36.000Z\\",\\"author\\":[]}"],["meta",{"property":"og:url","content":"https://velvet-notes.org/papers/%E7%BB%8F%E5%85%B8%E8%91%97%E4%BD%9C/transformer"}],["meta",{"property":"og:site_name","content":"Velvet-Notes"}],["meta",{"property":"og:title","content":"Attention Is All You Need"}],["meta",{"property":"og:description","content":"Transformer 概述 Transformer 解决的问题 Transformer 的 Tips Transformer 的模型结构如下："}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-05-25T13:13:36.000Z"}],["meta",{"property":"article:tag","content":"Self-Attention"}],["meta",{"property":"article:tag","content":"Transformer"}],["meta",{"property":"article:modified_time","content":"2025-05-25T13:13:36.000Z"}]]},"readingTime":{"minutes":4.22,"words":1267},"git":{"createdTime":1746782006000,"updatedTime":1748178816000,"contributors":[{"name":"Velvet","username":"Velvet0314","email":"3131492575@qq.com","commits":5,"avatar":"https://image.velvet-notes.org/blog/avatar.png","url":"https://github.com/Velvet0314"}]},"autoDesc":true,"filePathRelative":"papers/经典著作/transformer.md","headers":[],"categoryList":[{"id":"a566b2","sort":10000,"name":"papers"},{"id":"3111f2","sort":10003,"name":"经典著作"}]}');export{h as comp,g as data};
